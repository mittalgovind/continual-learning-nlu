%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym, amsmath}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

%%%%%%%%%%%%%%%%% GUIDELINES %%%%%%%%%%%%%%%
% The partial draft in a single document with three pages of content (not counting references, appendix, and contribution statement). This should essentially be a near-final draft of your paper (in ACL 2020 paper style) with the results, analysis, and conclusion sections missing. It should contain a link to your public GitHub repo, which should show evidence that you have implemented everything youâ€™ll need in order to complete your project.

% Introduction/overall idea (4):
% Clear overall motivation for project (2)
% Clear description and motivation for any novel techniques, including baseline results (2)
 
% Background (3):
% No major omissions from the literature review (1)
% 10+ papers cited (1)
% 5+ relevant papers discussed in detail, with explanations of how they relate to your project (1)
 
% Overall writing (2):
% Reasonably close fit to ACL style (1)
% Readable prose (1)
% Collaboration statement (0, -1 for groups that leave it out) [We reserve the right to redistribute points unevenly between team members in case of very uneven contributions.]

\newcommand\BibTeX{B{\sc ib}\TeX}
\usepackage[ruled,vlined]{algorithm2e}
\title{Continual NLU: Enabling language models to learn downstream tasks in an online fashion}

\author{{Mohith Damarapati*, Govind Mittal*, Chandra Prakash Konkimalla*, Aakriti Gupta}\\
  {\tt \{md4289, gm2724, cpk290, ag6817\}@nyu.edu}}
\date{}
\begin{document}
\maketitle
\begin{abstract}
  Qualified general intelligence has the ability of learning new tasks without immediately becoming worse at an older one. With this statement at the core of this paper, we explore various techniques to prevent catastrophic forgetting (CF) in a state-of-the-art language model. The paper explores three neuroscience inspired regularization techniques to mitigate CF, \textit{viz.}, Elastic Weight Consolidation, Memory Aware Synapses and Synaptic Intelligence on pre-trained BERT model. Two evaluation metrics -- backward and forward transfer, are used to evaluate the techniques. The paper aims to reduce the negative backward transfer for previous tasks and boost positive forward transfer for future tasks. As per our knowledge, this is the first work attempting to enable language models to learn downstream tasks in an online fashion and evaluating them based on the metrics mentioned above
\end{abstract}

% \section{credits

\section{Introduction}
\label{sec:intro}

Natural cognitive systems gradually forget previously learned information unlike the CF observed in current connectionist approaches \cite{french1999catastrophic}.  This problem is significant in the domain of natural language understanding (NLU) as well.  

Humans are remarkable at using their previously acquired knowledge when learning a new language task.  For instance, children first learn a language by mostly observing and listening to their parents.  They bolster their language skills through curricula by learning more formal language rules like grammar and syntax.  Using their linguistic knowledge, they can comprehend intricate texts and can answer questions based on them.  They can further infer meanings of natural language sentences used in complex contexts.  In all the stages of human learning, we are exceptionally good at retaining important parts of our previous knowledge.  

In order to design real-world self-learning systems, two requirements are essential -- adaptability to learn new knowledge and stability while evolving and integrating it. Therefore a the ratio of \textit{how plastic} and \textit{how static} a system is decides how adaptable and how stable it is, respectively. A balance needs to be maintained to alleviate the problem of CF of consolidated knowledge, which gives rise to the \textit{stability-plasticity dilemma}.



On the other hand, deep learning based language models drastically forget their previous knowledge while learning new tasks. Current state of the art language models, like BERT \cite{DBLP:journals/corr/abs-1810-04805}, struggle to learn multiple tasks in an online fashion \cite{DBLP:journals/corr/abs-1901-11373}. Despite having a huge capacity, these models cannot learn multiple tasks effectively due to CF.  

In the current literature, after pre-training a language model, it is fine-tuned on each downstream task separately. While evaluating a language model, performance metrics like accuracy and F1 score are reported separately for each downstream task. In this paper, we argue against this kind of evaluation of language models. Language models need to be evaluated on multiple tasks trained in a continual way and on metrics measuring knowledge retention capacity apart from accuracy and F1.  We further put forward an argument that retaining and reusing the linguistic knowledge and avoiding CF is an important characteristic of general linguistic intelligence.  

Our work in this paper mostly focuses on evaluating BERT-base trained on multiple tasks based on two metrics namely (i) backward transfer and (ii) forward transfer proposed in \citeauthor{DBLP:journals/corr/Lopez-PazR17}.  We compare vanilla fine-tuning of BERT-base with continual fine-turning based on neuroscience inspired techniques like Elastic Weight Consolidation (EWC, \citealt{DBLP:journals/corr/KirkpatrickPRVD16}), Memory Aware Synapses(MAS, \citealt{aljundi2018memory}) and Synaptic Intelligence (SI, \cite{Cortes2017AdaNetAS}).  As per our knowledge, this is the first work attempting to enable language models to learn downstream tasks in an online fashion and evaluating them based on the metrics mentioned above.

\section{Related Work}
\label{sec:related}

There has been multiple ways proposed to mitigate CF in connectionist networks. The paper segregates and reviews some of them into the following categories.

\subsection{Memory based methods}
Memory methods involve \textit{rehearsing} on old samples along with seeing new distribution data interleaved \cite{Robins1995CatastrophicFR, Gepperth2016ABI}. These methods although cure the forgetting better than other methods, but this involves storing past events explicitly, which makes it difficult to scale as the neural networks becomes very large. 
% which is the case for most state-of-the-art architectures 
% (e.g. BERT $\sim$ 340M parameters \cite{DBLP:journals/corr/abs-1810-04805}, FixEfficientNet $\sim$ 480M parameters \cite{touvron2019fixing}, etc). 
Also, this approach does not consider the fact that a network needs to learn unknown number of future tasks, which makes choosing \textit{defining set of examples} per task for storing in a fixed resourced system complicated.

\subsection{Regularization based methods}
These methods cure CF by enforcing a cost-based constraint during the update step of the network weights. These approaches aim to change a subset or all of the current parameters to an optimal value that works well for both the previous tasks and the current new task.

\citet{Li2018LearningWF} proposed \textit{Learning without Forgetting (LwF)} for convolutional neural networks. This approach optimizes the new task parameters, while additionally optimizing the predictions made by the shared parameters, does not deviate much from previous predictions. The loss function while optimizing for each new parameter, minimizes the difference between new output and old output predictions as though learning the old task again. 
% % The following equation illustrates the optimization step used in LwF, 

% \begin{equation}
% \begin{split}
% \theta^*_s,\theta^*_o,\theta^*_n \leftarrow \text{argmin}_{\hat{\theta}_s,\hat{\theta}_o,\hat{\theta}_n} [ \lambda_o \mathcal{L}_{old}(Y_o,\hat{Y}_o) \\ +
% \mathcal{L}_{new}(Y_n,\hat{Y}_n) + \mathcal{R}(\hat{\theta}_s,\hat{\theta}_o,\hat{\theta}_n)],
% \end{split}
% \end{equation}

% where $\mathcal{L}_{old}(Y_o,\hat{Y}_o)$ and $\mathcal{L}_{new}(Y_n,\hat{Y}_n)$ represent the difference between the predicted values $\hat{Y}$ and the ground truth values $Y$ of the old and new tasks, respectively using parameters $\hat{\theta}_s,\hat{\theta}_o $ and  $\hat{\theta}_n$. $\lambda_o$ is used to balance previous and current tasks, and $\mathcal{R}$ is a regularization term to prevent overfitting. 
After the optimization, a new smaller model is trained using knowledge distillation which works optimally as a multi-task model for all learned tasks. The requirement of storing data from previous tasks is its biggest disadvantage.

\citet{DBLP:journals/corr/KirkpatrickPRVD16} proposed  \textit{Elastic Weight Consolidation (EWC)} constrains each parameter based on how important it is in the original task, which results in model that is flexible for adapting to new tasks while maintaining the performance on previous task. Per-parameter significance is calculated using the diagonal of the Fischer information matrix, which is an approximation of the second derivative of loss with respect to each parameter. This approximation is linear to compute in the number of parameters and training examples.

The final parameters are calculated as follows:

\begin{equation}
\theta_N^* \leftarrow \text{argmin}_{\theta} L_N(\theta) + \sum_i \dfrac{\lambda}{2} (\dfrac{dL_{Pi}(\theta_{Pi})}{d\theta_{Pi}})^2(\theta - \theta_{Pi})^2 
\end{equation}

where $\theta_N^*$ are the optimal parameters after training on new task. $L_N(\theta)$ is the loss metric for new task, $dL_{Pi}(\theta_{Pi})$ is the loss metric for previous(i-th) task, $\theta_{Pi} $ are the optimal parameters until i-th task. $\lambda$ is the importance parameter for old tasks.


 \citet{aljundi2018memory} proposed \textit{Memory Aware Synapses} inspired by Hebbian learning in natural systems. The core idea is to learn which parts of the model parameters are important relative to others, using unlabelled data itself. They calculate importance weights $\Omega_{ij}$ for all parameters $\theta_{ij}$, where $i$ and $j$ are indices of pairs of neurons $n_i$ and $n_j$ residing in two consecutive layers, respectively. The importance weight represent the \textit{sensitivity} of the function output of a learned function, $F:X_n \longrightarrow Y_n$, to changes in the corresponding parameter. Therefore,

\begin{equation}
\Omega_{ij} =\frac{1}{N}\sum_{k=1}^N  \mid\mid g_{ij}(x_k) \mid\mid
\end{equation}
where, $N$ is the number of data points at a given stage and
\begin{equation}
g_{ij}(x_k)=\frac{\partial (F(x_k; \theta))}{\partial \theta_{ij}}
\end{equation}

for a given data point $x_k$. Therefore, whichever parameters have lower importance can be changed without degrading the accuracy on previous tasks, while decreasing the loss for subsequent tasks. Thus, if we wish to learn a new task, the total loss function $L(\theta)$ becomes,

\begin{equation}
L(\theta)=L_n(\theta)+{\lambda} \sum_{i,j} \Omega_{ij}(\theta_{ij}-\theta^*_{ij})^2
\end{equation}

where, $L_n(\theta)$ is the new task loss, $\theta^*_{ij}$ are the old parameters and $\lambda$ is a regularizer hyperparamter.


 \citet{DBLP:journals/corr/ZenkePG17} \textit{Synaptic Intelligence} loss is similar to MAS: 
 \begin{align*}
     L(\theta) &=L_n(\theta)+{\lambda} \sum_{k} \Omega_k^{\mu} (\theta - \theta_k)^2,\\ 
     where:
     \Omega_k^{\mu} &= \sum_{v<\mu} \dfrac{\omega_k^v}{(\Delta_k^v)^2+\epsilon} \\
     \omega_k^v &= \int_{t^{\mu-1}}^{t^{\mu}} g_k(t)\theta^{'}_k(t)
 \end{align*}
Here k signifies task number, $g_k(t)$ is the gradient of loss w.r.t parameter k, $\theta^{'}_k(t)$ is the parameter change at time t

In case of SI the importance of parameter is an accumulation of product of gradient and parameter change. It depends on two quantities: 1) $ \omega_k^v $ is a drop in loss over the trajectory of training. 2) How far the loss has changed $\Delta_k^v = \theta_k(t^v) - \theta_k(t^{v-1})$.


\subsection{Dynamic Architectures} 
In this approach, a network is augmented as new tasks and more neural resources are needed \cite{Rusu2016ProgressiveNN, Cortes2017AdaNetAS}. Old network parameters are fully preserved to prevent CF and additional neurons are added and trained separately. Although this approach works well for reinforcement learning tasks but leads to higher complexity of architecture as the number of learned tasks increases.
% proposed a model augmentation technique named \textit{Progressive Networks}, in which the old network parameters are preserved to prevent CF and when a new task needs to be learned, a new network is trained and coalesced into the old network by forming lateral connections. This approach although works well on reinforcement learning tasks, but leads 

% \citet{Cortes2017AdaNetAS} proposed a technique similar to neural architectural search named \textit{AdaNet}, in which they optimize both structure and weights of networks starting with a general structure. The models adapts itself as newer tasks are learned. The approach balances the model complexity by merging similar features to obtain compact feature representations and scaling up when more neural resources are needed.

\section{Proposed Method}
\label{sec:prop_method}

\subsection{Evaluation Metrics}
\label{sec:metric}
This paper uses two metrics proposed by \citeauthor{DBLP:journals/corr/Lopez-PazR17}. They are:
\begin{enumerate}
    \item \textit{Backward transfer (BWT)} is the influence of learning a new task $T$ on the previous $i$ tasks. 
    \item \textit{Forward transfer (FWT)} is the influence of learning a new task $T$ on future $i$ tasks.
\end{enumerate}

The paper defines $R$ as the accuracy matrix and $Tr$ as the transfer matrix, where $R_{i, j}$ is the test classification accuracy of the model on task $t_j$ after observing the last data point from task $t_i$ and $Tr_{i, j}}$ is the backward transfer when $i > j$ and the forward transfer when $i < j$, respectively.

\subsection{Continual Framework}

The paper proposes a continual framework for learning multiple downstream tasks on BERT and illustrates in Algorithm \ref{algo:1}.  Let $\theta_{B}$ be BERT pre-trained parameters\footnote{https://github.com/huggingface/transformers}.  In case of vanilla fine-tuning, for each task, task-specific head are initialized to zero and BERT-specific parameters are re-initialized to $\theta_{B}$.  But, in this framework, BERT-specific parameters are \textit{not re-initialized}, when training new tasks.   

Let us say we are fine-tuning BERT on $n$ downstream tasks ($T_i, 1<=i<=n $).  Before training the first task, we initialize BERT-specific parameters to $\theta_{B}$.  Since, we are not updating BERT-specific parameters, after training task $T_i$, let us say our BERT-specific parameters are  $\theta_{Bi}$.  When training a new task $T_{i+1}$, we attach freshly initialized task-specific head with parameters $\theta_{Ti}$ = $\theta_{Bi}$. When training a new task, $T_{i+1}$, we use techniques like EWC, MAS and SI to consolidate the previous BERT-specific parameters $\theta_{Bi}$. Consolidating previous parameters helps us retain knowledge obtained from previously trained tasks.

While evaluating our model, we report accuracy on task $T_i$ after re-installing its task-specific head.  More formally, after training a task $T_i$, we evaluate backward transfer for all previous tasks ($T_j, j<i$) and forward transfer for all future tasks ($T_j, j>i$).  We store backward and forward transfers in a $N \times N$ transfer matrix obtained from $R$.  Very high negative backward transfer is called catastrophic forgetting which is observed in vanilla fine-turning. But, using techniques like EWC, significantly reduced negative backward transfer.  Discussion about forward transfer and its significance is left to future work.    


\begin{algorithm}
    $T \leftarrow \mbox{Tasks}$ \\
    $N \leftarrow \mbox{Number of Tasks}$ \\
    $\theta_B \leftarrow \mbox{Bert Pre-trained Parameters}$\\
    $R \leftarrow 0_{N \times N}$ \Comment{;Test Accuracy Matrix}\\
    $Tr \leftarrow 0_{N \times N}$ \Comment{;Transfer Matrix}\\
  \For{$i = 1:N$}{
    $D_i \leftarrow data(T_{i, train})$ \\
    \theta_i = argmin_{\theta} L_t(D_i,  \theta_B; \theta_1, ..., \theta_{i-1})\\
    \theta_{B_i}, \theta_{T_i} = seperate(\theta_i) \\
    \For{$j = 1:T$}{
        $D_{test} \leftarrow data(T_{j, test})$ \\
        $R_{i,j} \leftarrow acc(\theta_{Bi}, \theta_{Tj}, D_{test})$ \\
        % \If{i \neq j}
        %     \State 
        $Tr_{i, j} \leftarrow R_{i, j} - R_{i, i}$ \\
        % \ElseIf{i $>$ j} 
        %     \State $Tr_{i, j} \leftarrow R_{j, i} - R_{i, i}$ \\
        % \EndIf % {}{\ifthenelse{i $>$ j}{}{}}
        % \\
    }
    }
    \textbf{return} $R, Tr$
    \caption{Proposed Framework}
    \label{algo:1}
\end{algorithm}

\subsection{Data}
\label{sec:data}

In this paper we work in three phases using various GLUE tasks \cite{wang2019glue}, as follows:
\begin{itemize}
  \item \textbf{First Phase (Two low data tasks):} Microsoft Research Paraphrase Corpus (MRPC) and Recognizing Textual Entailment (RTE).
  \item \textbf{Second Phase (Two high data tasks):} Quora Question Pairs (QQP) and Multi Natural Language Inference (MNLI), and vice-versa.
  \item \textbf{Final Phase (All tasks):} More than two tasks -- MRPC $\rightarrow$ RTE $\rightarrow$ MNLI-mm.
\end{itemize}

\section{Results}
\label{sec:results}

We obtained results for baseline and EWC, for the \textbf{first phase}. Our baseline results strengthen our hypothesis that the models suffer from very high negative backward transfer. Forward transfer is still low and methods to improve it will be discussed in the future work.  Table 1 shows the results on both vanilla fine-turning (baseline) and continual fine-turning with EWC.  The model trained with EWC has relatively lower negative backward transfer compared to baseline.  We are expecting similar results from MAS and SI.  

% Results for second phase final phase will also be shown in the future work. MAS and SI are work in progress.

% \subsection{First Phase}

% Baseline and EWC results are done for first phase. Accuracy and transfer for the first phase configuration (MRPC - RTE) is shown below. Results are reported after 2 epochs.

% \textbf{Baseline Matrix:}

% \begin{center}
%  \begin{tabular}{||c c c||} 
%  \hline
%  Task & MRPC & RTE  \\ [0.5ex] 
%  \hline\hline
%  MRPC & 0.86(CT) & 0.52(\textbf{FT} -0.12) \\ 
%  \hline
%  RTE & 0.65(\textbf{BT} -0.21) & 0.64(CT) \\ [1ex] 
%  \hline
% \end{tabular}
% \end{center}

% \textbf{EWC Matrix:}

% \begin{center}

% \end{center}


\begin{table}[h!]
\centering
 \begin{tabular}{|c|c|c|} 
 \hline
 \textbf{Task} & \textbf{MRPC} & \textbf{RTE}  \\
 \hline\hline
 & \textit{Baseline} & \\
 \hline
 \textbf{MRPC} & 0.86(CT) & 0.52(FT -0.12) \\ 
 \hline
 \textbf{RTE} & 0.65(BT -0.21) & 0.64(CT) \\ % [1ex] 
 \hline
 & \textit{EWC} & \\ 
 \hline\hline
 \textbf{MRPC} & 0.87(CT) & 0.52(FT -0.12) \\ 
 \hline
 \textbf{RTE} & 0.75(BT -0.12) & 0.64(CT) \\ %[1ex] 
 \hline
% \caption{Table A}
\end{tabular}

%  \begin{tabular}{|c|c|c|} 
%  \hline

% % \caption{Table A}
% \end{tabular}
% % \end{center}
\caption{\label{base-table} Results for baseline and EWC; \textbf{CT:} Current Task, \textbf{BT:} Backward Transfer, \textbf{FT:} Forward Transfer. 
 }
\end{table}

\section*{Availability}
The code and \LaTeX{} source files are available at \url{https://github.com/mittalgovind/continual-learning-nlu}.
% \section{Conclusion}
% \label{sec:conc}


\section*{Collaboration Statement}
The contributions of the authors, in the same order, are as follows:

\textit{Mohith Damarapati*:} Implemented EWC (Elastic Weight Consolidation) from scratch.  Designed a framework to evaluate models in a continual way (i.e. print both backward and forward transfers after training each task).  Fixed some crucial bugs that hindered our project progress. 

\textit{Govind Mittal*:} Implemented Memory Aware Synapses (MAS) for BERT from scratch. Wrote the complete related work (Sec. \ref{sec:related} and part of introduction (Sec. \ref{sec:intro}). 

\textit{Chandra Prakash Konkimalla*:} Implemented the baseline framework for finetuning the tasks one after another. Fixed bugs in EWC (Elastic Weight Consolidation) training procedure. Integrated Baseline and EWC into one code so that the results can be consistent. 

\textit{Aakriti Gupta:} No contribution to team yet. Still working on SI.

* = equal contribution
\bibliography{paper}
\bibliographystyle{acl_natbib}

% \section{Credits}

% This document has been adapted from the instructions
% for earlier ACL and NAACL proceedings,
% including 
% those for ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Michell and Stephanie Lukin,
% 2017/2018 (NA)ACL bibtex suggestions from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, 
% NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% those from ACL 2010 by Jing-Shing Chang and Philipp Koehn, 
% those for ACL 2008 by JohannaD. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% those for ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% those for ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats.
% Those versions were written by several
% people, including John Chen, Henry S. Thompson and Donald
% Walker. Additional elements were taken from the formatting
% instructions of the {\em International Joint Conference on Artificial
%   Intelligence} and the \emph{Conference on Computer Vision and
%   Pattern Recognition}.

% \section{Introduction}

% For example, when a human learns how to jump, they do not forget how to walk. 


% \section{General Instructions}

% Manuscripts must be in two-column format.  Exceptions to the
% two-column format include the title, authors' names and complete
% addresses, which must be centered at the top of the first page, and
% any full-width figures or tables (see the guidelines in
% Subsection~\ref{ssec:first}). {\bf Type single-spaced.}  Start all
% pages directly under the top margin. See the guidelines later
% regarding formatting the first page.  The manuscript should be
% printed single-sided and its length
% should not exceed the maximum page limit described in Section~\ref{sec:length}.
% Pages are numbered for  initial submission. However, {\bf do not number the pages in the camera-ready version}.

% By uncommenting {\small\verb|\aclfinalcopy|} at the top of this 
%  document, it will compile to produce an example of the camera-ready formatting; by leaving it commented out, the document will be anonymized for initial submission.  When you first create your submission on softconf, please fill in your submitted paper ID where {\small\verb|***|} appears in the {\small\verb|\def\aclpaperid{***}|} definition at the top.

% The review process is double-blind, so do not include any author information (names, addresses) when submitting a paper for review.  
% However, you should maintain space for names and addresses so that they will fit in the final (accepted) version.  The NAACL-HLT 2019 \LaTeX\ style will create a titlebox space of 2.5in for you when {\small\verb|\aclfinalcopy|} is commented out.  

% The author list for submissions should include all (and only) individuals who made substantial contributions to the work presented. Each author listed on a submission to NAACL-HLT 2019 will be notified of submissions, revisions and the final decision. No authors may be added to or removed from submissions to NAACL-HLT 2019 after the submission deadline.

% \subsection{The Ruler}
% The NAACL-HLT 2019 style defines a printed ruler which should be presented in the
% version submitted for review.  The ruler is provided in order that
% reviewers may comment on particular lines in the paper without
% circumlocution.  If you are preparing a document without the provided
% style files, please arrange for an equivalent ruler to
% appear on the final output pages.  The presence or absence of the ruler
% should not change the appearance of any other content on the page.  The
% camera ready copy should not contain a ruler. (\LaTeX\ users may uncomment the {\small\verb|\aclfinalcopy|} command in the document preamble.)  

% Reviewers: note that the ruler measurements do not align well with
% lines in the paper -- this turns out to be very difficult to do well
% when the paper contains many figures and equations, and, when done,
% looks ugly. In most cases one would expect that the approximate
% location will be adequate, although you can also use fractional
% references ({\em e.g.}, the first paragraph on this page ends at mark $108.5$).

% \subsection{Electronically-available resources}

% NAACL-HLT provides this description in \LaTeX2e{} ({\small\tt naaclhlt2019.tex}) and PDF
% format ({\small\tt naaclhlt2019.pdf}), along with the \LaTeX2e{} style file used to
% format it ({\small\tt naaclhlt2019.sty}) and an ACL bibliography style ({\small\tt acl\_natbib.bst})
% and example bibliography ({\small\tt naaclhlt2019.bib}).
% These files are all available at
% {\small\tt http://naacl2019.org/downloads/ naaclhlt2019-latex.zip}. 
%  We
% strongly recommend the use of these style files, which have been
% appropriately tailored for the NAACL-HLT 2019 proceedings.

% \subsection{Format of Electronic Manuscript}
% \label{sect:pdf}

% For the production of the electronic manuscript you must use Adobe's
% Portable Document Format (PDF). PDF files are usually produced from
% \LaTeX\ using the \textit{pdflatex} command. If your version of
% \LaTeX\ produces Postscript files, you can convert these into PDF
% using \textit{ps2pdf} or \textit{dvipdf}. On Windows, you can also use
% Adobe Distiller to generate PDF.

% Please make sure that your PDF file includes all the necessary fonts
% (especially tree diagrams, symbols, and fonts with Asian
% characters). When you print or create the PDF file, there is usually
% an option in your printer setup to include none, all or just
% non-standard fonts.  Please make sure that you select the option of
% including ALL the fonts. \textbf{Before sending it, test your PDF by
%   printing it from a computer different from the one where it was
%   created.} Moreover, some word processors may generate very large PDF
% files, where each page is rendered as an image. Such images may
% reproduce poorly. In this case, try alternative ways to obtain the
% PDF. One way on some systems is to install a driver for a postscript
% printer, send your document to the printer specifying ``Output to a
% file'', then convert the file to PDF.

% It is of utmost importance to specify the \textbf{A4 format} (21 cm
% x 29.7 cm) when formatting the paper. When working with
% {\tt dvips}, for instance, one should specify {\tt -t a4}.
% Or using the command \verb|\special{papersize=210mm,297mm}| in the latex
% preamble (directly below the \verb|\usepackage| commands). Then using 
% {\tt dvipdf} and/or {\tt pdflatex} which would make it easier for some.

% Print-outs of the PDF file on A4 paper should be identical to the
% hardcopy version. If you cannot meet the above requirements about the
% production of your electronic submission, please contact the
% publication chairs as soon as possible.

% \subsection{Layout}
% \label{ssec:layout}

% Format manuscripts two columns to a page, in the manner these
% instructions are formatted. The exact dimensions for a page on A4
% paper are:

% \begin{itemize}
% \item Left and right margins: 2.5 cm
% \item Top margin: 2.5 cm
% \item Bottom margin: 2.5 cm
% \item Column width: 7.7 cm
% \item Column height: 24.7 cm
% \item Gap between columns: 0.6 cm
% \end{itemize}

% \noindent Papers should not be submitted on any other paper size.
%  If you cannot meet the above requirements about the production of 
%  your electronic submission, please contact the publication chairs 
%  above as soon as possible.

% \subsection{Fonts}

% For reasons of uniformity, Adobe's {\bf Times Roman} font should be
% used. In \LaTeX2e{} this is accomplished by putting

% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \usepackage{latexsym}
% \end{verbatim}
% \end{quote}
% in the preamble. If Times Roman is unavailable, use {\bf Computer
%   Modern Roman} (\LaTeX2e{}'s default).  Note that the latter is about
%   10\% less dense than Adobe's Times Roman font.

% \begin{table}[t!]
% \begin{center}
% \begin{tabular}{|l|rl|}
% \hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
% paper title & 15 pt & bold \\
% author names & 12 pt & bold \\
% author affiliation & 12 pt & \\
% the word ``Abstract'' & 12 pt & bold \\
% section titles & 12 pt & bold \\
% document text & 11 pt  &\\
% captions & 10 pt & \\
% abstract text & 10 pt & \\
% bibliography & 10 pt & \\
% footnotes & 9 pt & \\
% \hline
% \end{tabular}
% \end{center}
% \caption{\label{font-table} Font guide. }
% \end{table}

% \subsection{The First Page}
% \label{ssec:first}

% Center the title, author's name(s) and affiliation(s) across both
% columns. Do not use footnotes for affiliations. Do not include the
% paper ID number assigned during the submission process. Use the
% two-column format only when you begin the abstract.

% {\bf Title}: Place the title centered at the top of the first page, in
% a 15-point bold font. (For a complete guide to font sizes and styles,
% see Table~\ref{font-table}) Long titles should be typed on two lines
% without a blank line intervening. Approximately, put the title at 2.5
% cm from the top of the page, followed by a blank line, then the
% author's names(s), and the affiliation on the following line. Do not
% use only initials for given names (middle initials are allowed). Do
% not format surnames in all capitals ({\em e.g.}, use ``Mitchell'' not
% ``MITCHELL'').  Do not format title and section headings in all
% capitals as well except for proper names (such as ``BLEU'') that are
% conventionally in all capitals.  The affiliation should contain the
% author's complete address, and if possible, an electronic mail
% address. Start the body of the first page 7.5 cm from the top of the
% page.

% The title, author names and addresses should be completely identical
% to those entered to the electronical paper submission website in order
% to maintain the consistency of author information among all
% publications of the conference. If they are different, the publication
% chairs may resolve the difference without consulting with you; so it
% is in your own interest to double-check that the information is
% consistent.

% {\bf Abstract}: Type the abstract at the beginning of the first
% column. The width of the abstract text should be smaller than the
% width of the columns for the text in the body of the paper by about
% 0.6 cm on each side. Center the word {\bf Abstract} in a 12 point bold
% font above the body of the abstract. The abstract should be a concise
% summary of the general thesis and conclusions of the paper. It should
% be no longer than 200 words. The abstract text should be in 10 point font.

% {\bf Text}: Begin typing the main body of the text immediately after
% the abstract, observing the two-column format as shown in the present document. Do not include page numbers.

% {\bf Indent}: Indent when starting a new paragraph, about 0.4 cm. Use 11 points for text and subsection headings, 12 points for section headings and 15 points for the title. 


% \begin{table}
% \centering
% \small
% \begin{tabular}{cc}
% \begin{tabular}{|l|l|}
% \hline
% {\bf Command} & {\bf Output}\\\hline
% \verb|{\"a}| & {\"a} \\
% \verb|{\^e}| & {\^e} \\
% \verb|{\`i}| & {\`i} \\ 
% \verb|{\.I}| & {\.I} \\ 
% \verb|{\o}| & {\o} \\
% \verb|{\'u}| & {\'u}  \\ 
% \verb|{\aa}| & {\aa}  \\\hline
% \end{tabular} & 
% \begin{tabular}{|l|l|}
% \hline
% {\bf Command} & {\bf  Output}\\\hline
% \verb|{\c c}| & {\c c} \\ 
% \verb|{\u g}| & {\u g} \\ 
% \verb|{\l}| & {\l} \\ 
% \verb|{\~n}| & {\~n} \\ 
% \verb|{\H o}| & {\H o} \\ 
% \verb|{\v r}| & {\v r} \\ 
% \verb|{\ss}| & {\ss} \\\hline
% \end{tabular}
% \end{tabular}
% \caption{Example commands for accented characters, to be used in, {\em e.g.}, \BibTeX\ names.}\label{tab:accents}
% \end{table}

% \subsection{Sections}

% {\bf Headings}: Type and label section and subsection headings in the
% style shown on the present document.  Use numbered sections (Arabic
% numerals) in order to facilitate cross references. Number subsections
% with the section number and the subsection number separated by a dot,
% in Arabic numerals.
% Do not number subsubsections.

% \begin{table*}[t!]
% \centering
% \begin{tabular}{lll}
%   output & natbib & previous ACL style files\\
%   \hline
%   \citep{Gusfield:97} & \verb|\citep| & \verb|\cite| \\
%   \citet{Gusfield:97} & \verb|\citet| & \verb|\newcite| \\
%   \citeyearpar{Gusfield:97} & \verb|\citeyearpar| & \verb|\shortcite| \\
% \end{tabular}
% \caption{Citation commands supported by the style file.
%   The citation style is based on the natbib package and
%   supports all natbib citation commands.
%   It also supports commands defined in previous ACL style files
%   for compatibility.
%   }
% \end{table*}

% {\bf Citations}: Citations within the text appear in parentheses
% as~\cite{Gusfield:97} or, if the author's name appears in the text
% itself, as Gusfield~\shortcite{Gusfield:97}.
% Using the provided \LaTeX\ style, the former is accomplished using
% {\small\verb|\cite|} and the latter with {\small\verb|\shortcite|} or {\small\verb|\newcite|}. Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}; this is accomplished with the provided style using commas within the {\small\verb|\cite|} command, {\em e.g.}, {\small\verb|\cite{Gusfield:97,Aho:72}|}. Append lowercase letters to the year in cases of ambiguities.  
%  Treat double authors as
% in~\cite{Aho:72}, but write as in~\cite{Chandra:81} when more than two
% authors are involved. Collapse multiple citations as
% in~\cite{Gusfield:97,Aho:72}. Also refrain from using full citations
% as sentence constituents.

% We suggest that instead of
% \begin{quote}
%   ``\cite{Gusfield:97} showed that ...''
% \end{quote}
% you use
% \begin{quote}
% ``Gusfield \shortcite{Gusfield:97}   showed that ...''
% \end{quote}

% If you are using the provided \LaTeX{} and Bib\TeX{} style files, you
% can use the command \verb|\citet| (cite in text)
% to get ``author (year)'' citations.

% If the Bib\TeX{} file contains DOI fields, the paper
% title in the references section will appear as a hyperlink
% to the DOI, using the hyperref \LaTeX{} package.
% To disable the hyperref package, load the style file
% with the \verb|nohyperref| option: \\{\small
% \verb|\usepackage[nohyperref]{naaclhlt2019}|}


% \textbf{Digital Object Identifiers}:  As part of our work to make ACL
% materials more widely used and cited outside of our discipline, ACL
% has registered as a CrossRef member, as a registrant of Digital Object
% Identifiers (DOIs), the standard for registering permanent URNs for
% referencing scholarly materials.  As of 2017, we are requiring all
% camera-ready references to contain the appropriate DOIs (or as a
% second resort, the hyperlinked ACL Anthology Identifier) to all cited
% works.  Thus, please ensure that you use Bib\TeX\ records that contain
% DOI or URLs for any of the ACL materials that you reference.
% Appropriate records should be found for most materials in the current
% ACL Anthology at \url{http://aclanthology.info/}.

% As examples, we cite \cite{P16-1001} to show you how papers with a DOI
% will appear in the bibliography.  We cite \cite{C14-1001} to show how
% papers without a DOI but with an ACL Anthology Identifier will appear
% in the bibliography.  

% As reviewing will be double-blind, the submitted version of the papers
% should not include the authors' names and affiliations. Furthermore,
% self-references that reveal the author's identity, {\em e.g.},
% \begin{quote}
% ``We previously showed \cite{Gusfield:97} ...''  
% \end{quote}
% should be avoided. Instead, use citations such as 
% \begin{quote}
% ``\citeauthor{Gusfield:97} \shortcite{Gusfield:97}
% previously showed ... ''
% \end{quote}

% Any preliminary non-archival versions of submitted papers should be listed in the submission form but not in the review version of the paper. NAACL-HLT 2019 reviewers are generally aware that authors may present preliminary versions of their work in other venues, but will not be provided the list of previous presentations from the submission form. 


% \textbf{Please do not use anonymous citations} and do not include
%  when submitting your papers. Papers that do not
% conform to these requirements may be rejected without review.

% \textbf{References}: Gather the full set of references together under
% the heading {\bf References}; place the section before any Appendices. 
% Arrange the references alphabetically
% by first author, rather than by order of occurrence in the text.
% By using a .bib file, as in this template, this will be automatically 
% handled for you. See the \verb|\bibliography| commands near the end for more.

% Provide as complete a citation as possible, using a consistent format,
% such as the one for {\em Computational Linguistics\/} or the one in the 
% {\em Publication Manual of the American 
% Psychological Association\/}~\cite{APA:83}. Use of full names for
% authors rather than initials is preferred. A list of abbreviations
% for common computer science journals can be found in the ACM 
% {\em Computing Reviews\/}~\cite{ACM:83}.

% The \LaTeX{} and Bib\TeX{} style files provided roughly fit the
% American Psychological Association format, allowing regular citations, 
% short citations and multiple citations as described above.  

% \begin{itemize}
% \item Example citing an arxiv paper: \cite{rasooli-tetrault-2015}. 
% \item Example article in journal citation: \cite{Ando2005}.
% \item Example article in proceedings, with location: \cite{borsch2011}.
% \item Example article in proceedings, without location: \cite{andrew2007scalable}.
% \end{itemize}
% See corresponding .bib file for further details.

% Submissions should accurately reference prior and related work, including code and data. If a piece of prior work appeared in multiple venues, the version that appeared in a refereed, archival venue should be referenced. If multiple versions of a piece of prior work exist, the one used by the authors should be referenced. Authors should not rely on automated citation indices to provide accurate references for prior and related work.

% {\bf Appendices}: Appendices, if any, directly follow the text and the
% references (but see above).  Letter them in sequence and provide an
% informative title: {\bf Appendix A. Title of Appendix}.

% \subsection{Footnotes}

% {\bf Footnotes}: Put footnotes at the bottom of the page and use 9
% point font. They may be numbered or referred to by asterisks or other
% symbols.\footnote{This is how a footnote should appear.} Footnotes
% should be separated from the text by a line.\footnote{Note the line
% separating the footnotes from the text.}

% \subsection{Graphics}

% {\bf Illustrations}: Place figures, tables, and photographs in the
% paper near where they are first discussed, rather than at the end, if
% possible.  Wide illustrations may run across both columns.  Color
% illustrations are discouraged, unless you have verified that  
% they will be understandable when printed in black ink.

% {\bf Captions}: Provide a caption for every illustration; number each one
% sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
% Caption of the Table.''  Type the captions of the figures and 
% tables below the body, using 10 point text. Captions should be placed below illustrations. Captions that are one line are centered (see Table~\ref{font-table}). Captions longer than one line are left-aligned (see Table~\ref{tab:accents}). Do not overwrite the default caption sizes. The naaclhlt2019.sty file is compatible with the caption and subcaption packages; do not add optional arguments.


% \subsection{Accessibility}
% \label{ssec:accessibility}

% In an effort to accommodate people who are color-blind (as well as those printing
% to paper), grayscale readability for all accepted papers will be
% encouraged.  Color is not forbidden, but authors should ensure that
% tables and figures do not rely solely on color to convey critical
% distinctions. A simple criterion: All curves and points in your figures should be clearly distinguishable without color.

% % Min: no longer used as of ACL 2018, following ACL exec's decision to
% % remove this extra workflow that was not executed much.
% % BEGIN: remove
% %% \section{XML conversion and supported \LaTeX\ packages}

% %% Following ACL 2014 we will also we will attempt to automatically convert 
% %% your \LaTeX\ source files to publish papers in machine-readable 
% %% XML with semantic markup in the ACL Anthology, in addition to the 
% %% traditional PDF format.  This will allow us to create, over the next 
% %% few years, a growing corpus of scientific text for our own future research, 
% %% and picks up on recent initiatives on converting ACL papers from earlier 
% %% years to XML. 

% %% We encourage you to submit a ZIP file of your \LaTeX\ sources along
% %% with the camera-ready version of your paper. We will then convert them
% %% to XML automatically, using the LaTeXML tool
% %% (\url{http://dlmf.nist.gov/LaTeXML}). LaTeXML has \emph{bindings} for
% %% a number of \LaTeX\ packages, including the ACL 2018 stylefile. These
% %% bindings allow LaTeXML to render the commands from these packages
% %% correctly in XML. For best results, we encourage you to use the
% %% packages that are officially supported by LaTeXML, listed at
% %% \url{http://dlmf.nist.gov/LaTeXML/manual/included.bindings}
% % END: remove

% \section{Translation of non-English Terms}

% It is also advised to supplement non-English characters and terms
% with appropriate transliterations and/or translations
% since not all readers understand all such characters and terms.
% Inline transliteration or translation can be represented in
% the order of: original-form transliteration ``translation''.

% \section{Length of Submission}
% \label{sec:length}

% The NAACL-HLT 2019 main conference accepts submissions of long papers and
% short papers.
%  Long papers may consist of up to eight (8) pages of
% content plus unlimited pages for references. Upon acceptance, final
% versions of long papers will be given one additional page -- up to nine (9)
% pages of content plus unlimited pages for references -- so that reviewers' comments
% can be taken into account. Short papers may consist of up to four (4)
% pages of content, plus unlimited pages for references. Upon
% acceptance, short papers will be given five (5) pages in the
% proceedings and unlimited pages for references. 
% For both long and short papers, all illustrations and tables that are part
% of the main text must be accommodated within these page limits, observing
% the formatting instructions given in the present document. Papers that do not conform to the specified length and formatting requirements are subject to be rejected without review.

% NAACL-HLT 2019 does encourage the submission of additional material that is relevant to the reviewers but not an integral part of the paper. There are two such types of material: appendices, which can be read, and non-readable supplementary materials, often data or code.  Do not include this additional material in the same document as your main paper. Additional material must be submitted as one or more separate files, and must adhere to the same anonymity guidelines as the main paper. The paper must be self-contained: it is optional for reviewers to look at the supplementary material. Papers should not refer, for further detail, to documents, code or data resources that are not available to the reviewers. Refer to Appendix~\ref{sec:appendix} and Appendix~\ref{sec:supplemental} for further information. 

% Workshop chairs may have different rules for allowed length and
% whether supplemental material is welcome. As always, the respective
% call for papers is the authoritative source.

% \section*{Acknowledgments}

% The acknowledgments should go immediately before the references.  Do
% not number the acknowledgments section. Do not include this section
% when submitting your paper for review. \\

% \noindent {\bf Preparing References:} \\
% Include your own bib file like this:
% \verb|\bibliographystyle{acl_natbib}|
% \verb|\bibliography{naaclhlt2019}| 

% where \verb|naaclhlt2019| corresponds to a naaclhlt2019.bib file.
% \bibliography{naaclhlt2019}
% \bibliographystyle{acl_natbib}

% \appendix

% \section{Appendices}
% \label{sec:appendix}
% Appendices are material that can be read, and include lemmas, formulas, proofs, and tables that are not critical to the reading and understanding of the paper. 
% Appendices should be {\bf uploaded as supplementary material} when submitting the paper for review. Upon acceptance, the appendices come after the references, as shown here. Use
% \verb|\appendix| before any appendix section to switch the section
% numbering over to letters.


% \section{Supplemental Material}
% \label{sec:supplemental}
% Submissions may include non-readable supplementary material used in the work and described in the paper. Any accompanying software and/or data should include licenses and documentation of research review as appropriate. Supplementary material may report preprocessing decisions, model parameters, and other details necessary for the replication of the experiments reported in the paper. Seemingly small preprocessing decisions can sometimes make a large difference in performance, so it is crucial to record such decisions to precisely characterize state-of-the-art methods. 

% Nonetheless, supplementary material should be supplementary (rather
% than central) to the paper. {\bf Submissions that misuse the supplementary 
% material may be rejected without review.}
% Supplementary material may include explanations or details
% of proofs or derivations that do not fit into the paper, lists of
% features or feature templates, sample inputs and outputs for a system,
% pseudo-code or source code, and data. (Source code and data should
% be separate uploads, rather than part of the paper).

% The paper should not rely on the supplementary material: while the paper
% may refer to and cite the supplementary material and the supplementary material will be available to the
% reviewers, they will not be asked to review the
% supplementary material.


\end{document}
